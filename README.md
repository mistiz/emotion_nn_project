# ğŸ­ Multimodal Emotion Detection System
A realâ€‘time multimodal emotion recognition system that analyzes **facial expressions** and **vocal tone** using deep learning, computer vision, and speech processing. The system uses your webcam and microphone to detect emotions live, fusing both modalities for more accurate and humanâ€‘like predictions.

---

## ğŸ“Œ Table of Contents
- [Overview](#overview)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Running the Application](#running-the-application)
- [How It Works](#how-it-works)
- [Models](#models)
- [Logging & Saved Outputs](#logging--saved-outputs)
- [Diagrams](#diagrams)
- [Requirements](#requirements)
- [Dataset](#dataset)
- [Future Improvements](#future-improvements)
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

## ğŸ§  Overview
This project performs **realâ€‘time emotion detection** by combining:
- CNNâ€‘based **facial emotion recognition**
- Whisperâ€‘based **speechâ€‘toâ€‘text**
- A **vocal emotion classifier**
- A **fusion engine** that merges both signals

The result is a more robust and realistic emotion detection system that mimics how humans interpret emotions.

---

## âœ… Features

### ğŸ¥ Facial Emotion Recognition
- Realâ€‘time webcam processing  
- Haar Cascade face detection  
- CNN model for emotion classification  
- Prediction smoothing for stability  

### ğŸ¤ Voice Emotion Recognition
- Whisper for speechâ€‘toâ€‘text  
- Vocal emotion classifier  
- Runs in a background thread  

### ğŸ”— Fusion Engine
- Combines face + voice emotion  
- Ruleâ€‘based decision logic  
- Handles mismatches and neutral cases  

### ğŸ–¥ï¸ User Interface
- Live webcam overlay  
- Displays:
  - Face emotion  
  - Voice emotion  
  - Fused emotion  
  - Whisper transcription  

### ğŸ“ Logging & Saving
- Logs all events to `emotion_log.txt`  
- Saves highâ€‘confidence face images to `saved_faces/`  

---

## ğŸ—ï¸ System Architecture

+-------------------------------------------------------------+ | Multimodal Emotion System | +-------------------------------------------------------------+ | | | +-------------------+ +---------------------------+ | | | UI Layer | | Logging Module | | | | (Webcam Overlay) | | (Events, Speech, Images) | | | +-------------------+ +---------------------------+ | | | | +-------------------------------------------------------+ | | | Application Controller | | | | (Main Loop, Threading, Fusion, Saving, Orchestration) | | | +--------------------+------------------+----------------+ | | | | | | +---------------+ +----------------+ | | | | | â–¼ â–¼ | | +-------------------+ +----------------------------+ | | | Face Processing | | Voice Processing | | | | (Haar, CNN, Smooth)| | (Whisper, Vocal Emotion) | | | +-------------------+ +----------------------------+ | | | +-------------------------------------------------------------+

Code

---

## ğŸ“ Project Structure

emotion_nn_project/ â”‚ â”œâ”€â”€ run_emotion_detector.py â”œâ”€â”€ audio_emotion.py â”œâ”€â”€ fusion_logic.py â”œâ”€â”€ requirements.txt â”œâ”€â”€ README.md â”œâ”€â”€ .gitignore â”œâ”€â”€ .gitattributes â”‚ â”œâ”€â”€ models/ â”‚ â”œâ”€â”€ emotion_model.h5 â”‚ â”œâ”€â”€ emotion_model.weights.h5 â”‚ â””â”€â”€ haarcascade_frontalface_default.xml â”‚ â”œâ”€â”€ launch/ â”‚ â”œâ”€â”€ run_emotion.bat â”‚ â””â”€â”€ setup_env.bat â”‚ â”œâ”€â”€ diagrams/ â”‚ â”œâ”€â”€ architecture.png â”‚ â”œâ”€â”€ component_diagram.png â”‚ â””â”€â”€ fusion_flowchart.png â”‚ â”œâ”€â”€ slides/ â”‚ â””â”€â”€ presentation.pptx â”‚ â””â”€â”€ docs/ â”œâ”€â”€ system_overview.md â”œâ”€â”€ installation_guide.md â””â”€â”€ usage_guide.md

Code

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/emotion_nn_project.git
cd emotion_nn_project
2ï¸âƒ£ Create and activate a virtual environment
bash
python -m venv venv
venv\Scripts\activate
3ï¸âƒ£ Install dependencies
bash
pip install -r requirements.txt
â–¶ï¸ Running the Application
âœ… Option 1 â€” Oneâ€‘click launcher
bash
launch/run_emotion.bat
âœ… Option 2 â€” Manual run
bash
python run_emotion_detector.py
ğŸ” How It Works
1. Capture
Webcam captures video

Microphone captures audio

2. Analyze
Face â†’ Haar Cascade â†’ CNN â†’ Emotion

Voice â†’ Whisper â†’ Vocal Emotion

3. Fuse
Ruleâ€‘based fusion engine combines both signals

4. Display
Live overlay with all predictions

5. Log & Save
Logs events

Saves highâ€‘confidence face images

ğŸ§© Models
âœ… Facial Emotion Model
emotion_model.h5

emotion_model.weights.h5

âœ… Face Detection
haarcascade_frontalface_default.xml

âœ… Whisper
Installed via pip

No model files stored in repo

ğŸ“ Logging & Saved Outputs
Logs stored in:

Code
emotion_log.txt
Saved face images stored in:

Code
saved_faces/
Both are excluded from GitHub via .gitignore.

ğŸ–¼ï¸ Diagrams
All diagrams are stored in:

Code
diagrams/
Includes:

Architecture Diagram

Component Diagram

Data Flow Diagram

Fusion Flowchart

ğŸ“¦ Requirements
All dependencies are listed in:

Code
requirements.txt
Key libraries:

TensorFlow / Keras

OpenCV

Whisper

NumPy

SciPy

PyAudio or sounddevice

ğŸ“š Dataset
This project uses:

FER2013 for facial emotion training

A small curated dataset for vocal emotion (optional)

âš ï¸ Datasets are not included due to size.

ğŸš€ Future Improvements
Transformerâ€‘based facial emotion models

MLâ€‘based fusion

Multiâ€‘face detection and tracking

GUI dashboard (Tkinter / PyQt)

REST API for remote emotion detection

Analytics mode for emotion timelines

ğŸ“„ License
MIT License You are free to use, modify, and distribute this project.

ğŸ™Œ Acknowledgements
TensorFlow

OpenCV

Whisper

FER2013 dataset

Python community


